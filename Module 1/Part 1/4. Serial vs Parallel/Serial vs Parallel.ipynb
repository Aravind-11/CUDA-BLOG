{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55346660-8ea2-4ada-adac-0b97d7e5acc9",
   "metadata": {},
   "source": [
    "# Serial vs Parallel Computation\n",
    "\n",
    "\n",
    "<img src=\"./images/serial.svg\" width=\"800\" height=\"600\">\n",
    "image credits : claude.ai\n",
    "\n",
    "## Content\n",
    "* [Serial Particle Updates](#Serial-Particle-Updates)\n",
    "* [Parallel Particle Updates](#Parallel-Particle-Updates) \n",
    "* [Exercise: Particle Update Optimization](#Exercise-Particle-Update-Optimization)\n",
    "* [Exercise: Particle Forces](#Exercise-Particle-Forces)\n",
    "\n",
    "---\n",
    "\n",
    "At this point, we have discussed how to on-ramp to GPU programming with parallel algorithms.\n",
    "We've also covered techniques that can help you extend these parallel algorithms to meet your specific use cases.\n",
    "As you find more applications for parallel algorithms, there's a possibility that you will get unexpected performance.\n",
    "To avoid unexpected performance results, you'll need a firm understanding of the difference between serial and parallel execution.\n",
    "Let's explore this through particle simulation.\n",
    "\n",
    "## Serial Particle Updates\n",
    "\n",
    "In a particle simulation system, we need to update the positions of many particles based on their velocities.\n",
    "Let's first look at a serial implementation where we process each particle one at a time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bd3bbd-3ffd-4ad0-b0c3-b242ae627778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying path to where nvcc exists so that the jupyter notebook reads from it. nvcc is the nvidia cuda compiler for executing cuda. \n",
    "import os\n",
    "os.environ['PATH'] = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.6.1-cf4xlcbcfpwchqwo5bktxyhjagryzcx6/bin:\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed710361-952f-4823-8b3f-c1ceeed5afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/serial_particles.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/serial_particles.cu\n",
    "\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <cstdio>\n",
    "#include <chrono>\n",
    "\n",
    "void update_particles_serial(int num_particles, float dt,\n",
    "                           const thrust::universal_vector<float> &in,\n",
    "                                 thrust::universal_vector<float> &out) {\n",
    "    const float *in_ptr = thrust::raw_pointer_cast(in.data());\n",
    "    float *out_ptr = thrust::raw_pointer_cast(out.data());\n",
    "    \n",
    "    // Process each particle sequentially\n",
    "    for(int i = 0; i < num_particles; i++) {\n",
    "        // Update x position\n",
    "        out_ptr[i*4 + 0] = in_ptr[i*4 + 0] + dt * in_ptr[i*4 + 2];\n",
    "        // Update y position\n",
    "        out_ptr[i*4 + 1] = in_ptr[i*4 + 1] + dt * in_ptr[i*4 + 3];\n",
    "        // Keep velocities unchanged\n",
    "        out_ptr[i*4 + 2] = in_ptr[i*4 + 2];\n",
    "        out_ptr[i*4 + 3] = in_ptr[i*4 + 3];\n",
    "    }\n",
    "}\n",
    "\n",
    "thrust::universal_vector<float> init_particles(int num_particles) {\n",
    "    thrust::universal_vector<float> particles(num_particles * 4);\n",
    "    for(int i = 0; i < num_particles; i++) {\n",
    "        particles[i*4 + 0] = i * 0.1f;     // x position\n",
    "        particles[i*4 + 1] = i * -0.1f;    // y position\n",
    "        particles[i*4 + 2] = 1.0f;         // x velocity\n",
    "        particles[i*4 + 3] = -0.5f;        // y velocity\n",
    "    }\n",
    "    return particles;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int num_particles = 1000000;  // 1 million particles\n",
    "    float dt = 0.1f;\n",
    "    \n",
    "    // Initialize particles\n",
    "    thrust::universal_vector<float> particles = init_particles(num_particles);\n",
    "    thrust::universal_vector<float> output(particles.size());\n",
    "    \n",
    "    // Measure performance\n",
    "    auto begin = std::chrono::high_resolution_clock::now();\n",
    "    update_particles_serial(num_particles, dt, particles, output);\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    const double seconds = std::chrono::duration<double>(end - begin).count();\n",
    "    const double gigabytes = static_cast<double>(particles.size() * sizeof(float)) / 1024 / 1024 / 1024;\n",
    "    const double throughput = gigabytes / seconds;\n",
    "\n",
    "    std::printf(\"Serial computation:\\n\");\n",
    "    std::printf(\"Time: %g seconds\\n\", seconds);\n",
    "    std::printf(\"Throughput: %g GB/s\\n\", throughput);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca5b2b-8970-4a41-a07e-280b384c3a5b",
   "metadata": {},
   "source": [
    "Let's analyze the performance of this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068e466f-b7a2-4168-a57c-d23a7b8c0ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial computation:\n",
      "Time: 0.0118792 seconds\n",
      "Throughput: 1.25439 GB/s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvcc -o codes/serial_particles --extended-lambda codes/serial_particles.cu\n",
    "./codes/serial_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b86f0-9cd3-44d1-8888-0a51e669d6e1",
   "metadata": {},
   "source": [
    "## Parallel Particle Updates\n",
    "\n",
    "GPUs are massively parallel processors. The serial implementation processes particles one at a time, which doesn't take advantage of the GPU's parallel processing capabilities. Let's transform this into a parallel implementation using `thrust::transform`:\n",
    "\n",
    "\n",
    "This parallel implementation:\n",
    "1. Uses `thrust::transform` to process all elements simultaneously\n",
    "2. Uses mdspan for clearer data access\n",
    "3. Updates positions in parallel while keeping velocities unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47338403-688e-41de-9cd3-b5b4f78b4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/parallel_particles.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/parallel_particles.cu\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <cuda/std/mdspan>\n",
    "#include <cstdio>\n",
    "#include <chrono>\n",
    "\n",
    "// Serial implementation\n",
    "void update_particles_serial(int num_particles, float dt,\n",
    "                           const thrust::universal_vector<float> &in,\n",
    "                                 thrust::universal_vector<float> &out) {\n",
    "    const float *in_ptr = thrust::raw_pointer_cast(in.data());\n",
    "    float *out_ptr = thrust::raw_pointer_cast(out.data());\n",
    "    \n",
    "    // Process each particle sequentially\n",
    "    for(int i = 0; i < num_particles; i++) {\n",
    "        // Update x position\n",
    "        out_ptr[i*4 + 0] = in_ptr[i*4 + 0] + dt * in_ptr[i*4 + 2];\n",
    "        // Update y position\n",
    "        out_ptr[i*4 + 1] = in_ptr[i*4 + 1] + dt * in_ptr[i*4 + 3];\n",
    "        // Keep velocities unchanged\n",
    "        out_ptr[i*4 + 2] = in_ptr[i*4 + 2];\n",
    "        out_ptr[i*4 + 3] = in_ptr[i*4 + 3];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Parallel implementation\n",
    "void update_particles_parallel(int num_particles, float dt,\n",
    "                             const thrust::universal_vector<float> &in,\n",
    "                                   thrust::universal_vector<float> &out) {\n",
    "    const float *in_ptr = thrust::raw_pointer_cast(in.data());\n",
    "    \n",
    "    thrust::transform(\n",
    "        thrust::device,\n",
    "        thrust::counting_iterator<int>(0),\n",
    "        thrust::counting_iterator<int>(num_particles * 4),\n",
    "        out.begin(),\n",
    "        [=] __device__ (int idx) {\n",
    "            int particle_idx = idx / 4;\n",
    "            int component = idx % 4;\n",
    "            \n",
    "            if (component < 2) {  // Position components\n",
    "                return in_ptr[idx] + dt * in_ptr[particle_idx * 4 + component + 2];\n",
    "            }\n",
    "            return in_ptr[idx];  // Velocity components unchanged\n",
    "        }\n",
    "    );\n",
    "}\n",
    "\n",
    "thrust::universal_vector<float> init_particles(int num_particles) {\n",
    "    thrust::universal_vector<float> particles(num_particles * 4);\n",
    "    for(int i = 0; i < num_particles; i++) {\n",
    "        particles[i*4 + 0] = i * 0.1f;     // x position\n",
    "        particles[i*4 + 1] = i * -0.1f;    // y position\n",
    "        particles[i*4 + 2] = 1.0f;         // x velocity\n",
    "        particles[i*4 + 3] = -0.5f;        // y velocity\n",
    "    }\n",
    "    return particles;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int num_particles = 1000000;  // 1 million particles\n",
    "    float dt = 0.1f;\n",
    "    \n",
    "    // Initialize particles\n",
    "    thrust::universal_vector<float> particles = init_particles(num_particles);\n",
    "    thrust::universal_vector<float> output(particles.size());\n",
    "    \n",
    "    // Measure serial performance\n",
    "    auto begin = std::chrono::high_resolution_clock::now();\n",
    "    update_particles_serial(num_particles, dt, particles, output);\n",
    "    auto end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    double seconds = std::chrono::duration<double>(end - begin).count();\n",
    "    double gigabytes = static_cast<double>(particles.size() * sizeof(float)) / 1024 / 1024 / 1024;\n",
    "    double throughput = gigabytes / seconds;\n",
    "\n",
    "    std::printf(\"Serial computation:\\n\");\n",
    "    std::printf(\"Time: %g seconds\\n\", seconds);\n",
    "    std::printf(\"Throughput: %g GB/s\\n\\n\", throughput);\n",
    "    \n",
    "    // Measure parallel performance\n",
    "    begin = std::chrono::high_resolution_clock::now();\n",
    "    update_particles_parallel(num_particles, dt, particles, output);\n",
    "    end = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    seconds = std::chrono::duration<double>(end - begin).count();\n",
    "    throughput = gigabytes / seconds;\n",
    "\n",
    "    std::printf(\"Parallel computation:\\n\");\n",
    "    std::printf(\"Time: %g seconds\\n\", seconds);\n",
    "    std::printf(\"Throughput: %g GB/s\\n\", throughput);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65cb676-756e-4836-b2d0-f6cfb562fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial computation:\n",
      "Time: 0.00984519 seconds\n",
      "Throughput: 1.51355 GB/s\n",
      "\n",
      "Parallel computation:\n",
      "Time: 0.00980984 seconds\n",
      "Throughput: 1.519 GB/s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvcc -o codes/parallel_particles --extended-lambda codes/parallel_particles.cu\n",
    "./codes/parallel_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bc684-344b-46d2-8261-093958c3c769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
