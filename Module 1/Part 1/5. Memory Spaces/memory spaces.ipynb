{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4ad6fe-5477-42ad-8b56-cda2d7e4b9d3",
   "metadata": {},
   "source": [
    "# Memory Spaces in GPU Programming\n",
    "\n",
    "## Content\n",
    "* [Host and Device Memory Basics](#Host-and-Device-Memory-Basics)\n",
    "* [Universal Vector vs Explicit Memory Management](#Universal-Vector-vs-Explicit-Memory-Management)\n",
    "* [Performance Impact of Memory Transfers](#Performance-Impact-of-Memory-Transfers)\n",
    "\n",
    "Let's explore how memory spaces work in GPU programming using our particle simulation system. First, let's understand why memory spaces matter for performance.\n",
    "\n",
    "GPUs achieve their massive parallelism partly through specialized high-bandwidth memory. While CPUs prioritize low latency memory access, GPUs focus on high throughput to support thousands of concurrent threads. This is why GPUs typically have their own dedicated memory rather than just using system RAM.\n",
    "\n",
    "Let's see how this affects our particle simulation code.\n",
    "\n",
    "## Host and Device Memory Basics\n",
    "\n",
    "Here's a simple example demonstrating the different memory spaces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7fac9a-25c5-44ff-87ee-fe894f1f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying path to where nvcc exists so that the jupyter notebook reads from it. nvcc is the nvidia cuda compiler for executing cuda. \n",
    "import os\n",
    "os.environ['PATH'] = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.6.1-cf4xlcbcfpwchqwo5bktxyhjagryzcx6/bin:\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df6471b-c366-4cf8-b898-b2c6e2d28770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/example.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/example.cu \n",
    "\n",
    "// Host memory - accessible by CPU\n",
    "thrust::host_vector<float> h_positions{0.1f, 0.2f, 0.3f, 0.4f};  // x,y positions\n",
    "\n",
    "// Device memory - accessible by GPU\n",
    "thrust::device_vector<float> d_positions(4);\n",
    "\n",
    "// Copy data from host to device\n",
    "thrust::copy(h_positions.begin(), h_positions.end(), d_positions.begin());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609bc437-ddec-486a-afd2-750d8758033f",
   "metadata": {},
   "source": [
    "In this code:\n",
    "- `host_vector` allocates memory in CPU/host memory space\n",
    "- `device_vector` allocates memory in GPU/device memory space \n",
    "- We must explicitly copy data between spaces\n",
    "## Universal Vector vs Explicit Memory Management\n",
    "\n",
    "Let's look at two versions of our particle simulation code - one using universal_vector and one with explicit memory management:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d756d72-6437-4d31-8f0f-fae6fb00012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/example.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/example.cu \n",
    "\n",
    "// Version 1: Using universal_vector (automatic but potentially inefficient)\n",
    "void simulate_particles_v1() {\n",
    "    thrust::universal_vector<float> positions = init_particles(1000000);\n",
    "    thrust::universal_vector<float> forces(positions.size());\n",
    "    \n",
    "    for(int step = 0; step < 100; step++) {\n",
    "        // GPU computation\n",
    "        compute_forces(positions, forces);\n",
    "        \n",
    "        // CPU visualization every 10 steps\n",
    "        if(step % 10 == 0) {\n",
    "            visualize_particles(positions);  // Implicit transfer to host!\n",
    "        }\n",
    "        \n",
    "        update_positions(positions, forces);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Version 2: Explicit memory management (more control, better performance)\n",
    "void simulate_particles_v2() {\n",
    "    // Host vectors for CPU operations\n",
    "    thrust::host_vector<float> h_positions = init_particles(1000000);\n",
    "    thrust::host_vector<float> h_forces(h_positions.size());\n",
    "    \n",
    "    // Device vectors for GPU operations\n",
    "    thrust::device_vector<float> d_positions = h_positions;\n",
    "    thrust::device_vector<float> d_forces(d_positions.size());\n",
    "    \n",
    "    for(int step = 0; step < 100; step++) {\n",
    "        // GPU computation using device memory\n",
    "        compute_forces(d_positions, d_forces);\n",
    "        \n",
    "        // Only copy to host when needed for visualization\n",
    "        if(step % 10 == 0) {\n",
    "            thrust::copy(d_positions.begin(), d_positions.end(), \n",
    "                        h_positions.begin());\n",
    "            visualize_particles(h_positions);\n",
    "        }\n",
    "        \n",
    "        update_positions(d_positions, d_forces);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4482ad-30d2-4f6d-a578-44d75c7a5b0f",
   "metadata": {},
   "source": [
    "Let's measure the impact of memory transfers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a860f714-c69a-48ff-92e4-a6b990ab38ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/example.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/example.cu \n",
    "\n",
    "#include <thrust/host_vector.h>\n",
    "#include <thrust/device_vector.h>\n",
    "#include <thrust/universal_vector.h>\n",
    "#include <thrust/copy.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <chrono>\n",
    "#include <cstdio>\n",
    "#include <cmath>\n",
    "\n",
    "namespace {\n",
    "\n",
    "// Simple particle physics computation on GPU\n",
    "__global__ void compute_forces_kernel(float* data, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= n) return;\n",
    "    \n",
    "    // Access particle data\n",
    "    float x = data[idx * 4 + 0];\n",
    "    float y = data[idx * 4 + 1];\n",
    "    \n",
    "    // Simple force calculation (distance from origin)\n",
    "    float r = sqrtf(x*x + y*y);\n",
    "    float force = 1.0f / (r + 0.1f);  // Avoid division by zero\n",
    "    \n",
    "    // Store forces\n",
    "    data[idx * 4 + 2] = -force * x/r;  // fx\n",
    "    data[idx * 4 + 3] = -force * y/r;  // fy\n",
    "}\n",
    "\n",
    "// Helper function to run kernel\n",
    "template<typename VectorType>\n",
    "void compute_forces(VectorType& data) {\n",
    "    int n = data.size() / 4;\n",
    "    float* raw_ptr = thrust::raw_pointer_cast(data.data());\n",
    "    \n",
    "    int block_size = 256;\n",
    "    int num_blocks = (n + block_size - 1) / block_size;\n",
    "    compute_forces_kernel<<<num_blocks, block_size>>>(raw_ptr, n);\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "\n",
    "// Simple benchmark comparing memory transfer approaches\n",
    "void measure_transfer_overhead() {\n",
    "    const int N = 1000000;  // 1 million particles\n",
    "    \n",
    "    // Allocate data\n",
    "    thrust::host_vector<float> h_data(N * 4);\n",
    "    thrust::device_vector<float> d_data = h_data;\n",
    "    \n",
    "    // Fill with some data\n",
    "    for(int i = 0; i < N * 4; i++) {\n",
    "        h_data[i] = static_cast<float>(i);\n",
    "    }\n",
    "    \n",
    "    // Test 1: Compute only\n",
    "    auto t1 = std::chrono::high_resolution_clock::now();\n",
    "    compute_forces(d_data);\n",
    "    auto t2 = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    // Test 2: Compute + transfers \n",
    "    auto t3 = std::chrono::high_resolution_clock::now();\n",
    "    thrust::copy(d_data.begin(), d_data.end(), h_data.begin());\n",
    "    compute_forces(d_data);\n",
    "    thrust::copy(d_data.begin(), d_data.end(), h_data.begin());\n",
    "    auto t4 = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    double compute_time = std::chrono::duration<double>(t2-t1).count();\n",
    "    double total_time = std::chrono::duration<double>(t4-t3).count();\n",
    "    \n",
    "    printf(\"\\nMemory Transfer Analysis:\\n\");\n",
    "    printf(\"Compute only: %g seconds\\n\", compute_time);\n",
    "    printf(\"With transfers: %g seconds\\n\", total_time);\n",
    "    printf(\"Transfer overhead: %g seconds\\n\", total_time - compute_time);\n",
    "}\n",
    "\n",
    "// Demonstrate universal vector vs explicit memory management\n",
    "void compare_memory_approaches() {\n",
    "    const int N = 100000;  // 100k particles\n",
    "    printf(\"\\nComparing memory management approaches with %d particles...\\n\", N);\n",
    "    \n",
    "    // Test 1: Universal vector (automatic transfers)\n",
    "    {\n",
    "        thrust::universal_vector<float> data(N * 4, 1.0f);\n",
    "        \n",
    "        auto t1 = std::chrono::high_resolution_clock::now();\n",
    "        \n",
    "        // Do some work with automatic memory management\n",
    "        for(int i = 0; i < 10; i++) {\n",
    "            compute_forces(data);\n",
    "            // Simulate CPU work every few iterations\n",
    "            if(i % 3 == 0) {\n",
    "                float sum = thrust::reduce(thrust::host, data.begin(), data.end());\n",
    "                if(sum < -1e10) printf(\"Unlikely sum: %f\\n\", sum);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        auto t2 = std::chrono::high_resolution_clock::now();\n",
    "        double time = std::chrono::duration<double>(t2-t1).count();\n",
    "        printf(\"Universal vector time: %g seconds\\n\", time);\n",
    "    }\n",
    "    \n",
    "    // Test 2: Explicit memory management\n",
    "    {\n",
    "        thrust::host_vector<float> h_data(N * 4, 1.0f);\n",
    "        thrust::device_vector<float> d_data = h_data;\n",
    "        \n",
    "        auto t1 = std::chrono::high_resolution_clock::now();\n",
    "        \n",
    "        // Do same work with explicit memory management\n",
    "        for(int i = 0; i < 10; i++) {\n",
    "            compute_forces(d_data);\n",
    "            // Only transfer when needed\n",
    "            if(i % 3 == 0) {\n",
    "                thrust::copy(d_data.begin(), d_data.end(), h_data.begin());\n",
    "                float sum = thrust::reduce(thrust::host, h_data.begin(), h_data.end());\n",
    "                if(sum < -1e10) printf(\"Unlikely sum: %f\\n\", sum);\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        auto t2 = std::chrono::high_resolution_clock::now();\n",
    "        double time = std::chrono::duration<double>(t2-t1).count();\n",
    "        printf(\"Explicit memory time: %g seconds\\n\", time);\n",
    "    }\n",
    "}\n",
    "\n",
    "} // anonymous namespace\n",
    "\n",
    "int main() {\n",
    "    // Show overhead of memory transfers\n",
    "    measure_transfer_overhead();\n",
    "    \n",
    "    // Compare different memory management approaches\n",
    "    compare_memory_approaches();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ee57240-9e08-48f5-8a1f-fc8dfbbc7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Transfer Analysis:\n",
      "Compute only: 0.348636 seconds\n",
      "With transfers: 0.00996045 seconds\n",
      "Transfer overhead: -0.338675 seconds\n",
      "\n",
      "Comparing memory management approaches with 100000 particles...\n",
      "Universal vector time: 0.159094 seconds\n",
      "Explicit memory time: 0.0881803 seconds\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvcc -o codes/example --extended-lambda codes/example.cu \n",
    "./codes/example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75e31f-6610-47f5-ae5f-18971818f2e4",
   "metadata": {},
   "source": [
    "Key takeaways:\n",
    "1. GPU has its own memory space optimized for parallel access\n",
    "2. Data must be explicitly moved between CPU and GPU memory\n",
    "3. `universal_vector` provides convenience but may cause hidden transfers\n",
    "4. Memory transfers can significantly impact performance\n",
    "5. Best practice: Keep data in GPU memory when doing repeated GPU operations\n",
    "\n",
    "For optimal performance:\n",
    "- Minimize host-device transfers\n",
    "- Batch operations to avoid frequent transfers\n",
    "- Only transfer data when absolutely necessary\n",
    "- Keep compute-intensive operations on the GPU\n",
    "- Use explicit memory management for better control\n",
    "\n",
    "Next up we'll do an exercise to practice these concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17a9fb-1980-4631-92ef-41ccf95dd568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
