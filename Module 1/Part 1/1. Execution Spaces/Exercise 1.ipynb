{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21aae36d-4375-4e29-ac05-e91bc47b5720",
   "metadata": {},
   "source": [
    "# Practice Exercise: Where Does Your CUDA Code Run?\n",
    "\n",
    "In this notebook, we'll practice identifying where code executes in a CUDA program. Understanding execution spaces is a fundamental concept that will help you write effective GPU code.\n",
    "\n",
    "<img src=\"./images/image3.jpg\" width=\"800\" height=\"600\">\n",
    "\n",
    "[image credits](https://unsplash.com/illustrations/a-black-and-white-photo-of-a-keyboard-poUqDRe3Z7U)\n",
    "\n",
    "# Introduction to Execution Spaces\n",
    "\n",
    "When writing CUDA programs, you need to be conscious of where each line of code runs:\n",
    "\n",
    "- **CPU (Host)**: The main processor that coordinates computation\n",
    "\n",
    "- **GPU (Device)**: The accelerator that performs parallel computation\n",
    "\n",
    "A common misconception is that simply using the CUDA compiler (NVCC) makes code run on the GPU. The reality is that you must explicitly specify which parts of your code should run on the GPU.\n",
    "\n",
    "# Exercise: Identifying Execution Spaces\n",
    "\n",
    "Let's see if you can determine where different parts of a program execute. In the following code, we've used a helper function called print_location() that will tell us where code is running.\n",
    "\n",
    "Your task is to replace each \"???\" with either \"CPU\" or \"GPU\" based on where you think that part of the code executes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639f4743-c432-4e08-94d5-1af9001dbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying path to where nvcc exists so that the jupyter notebook reads from it. nvcc is the nvidia cuda compiler for executing cuda. \n",
    "import os\n",
    "os.environ['PATH'] = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/cuda-12.6.1-cf4xlcbcfpwchqwo5bktxyhjagryzcx6/bin:\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee13bc3b-e503-44c3-a5c6-f0cc5ae7d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/particle_locations.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/particle_locations.cu\n",
    "#include <thrust/execution_policy.h>\n",
    "#include <cstdio>\n",
    "\n",
    "// Helper function to print execution location, callable from both host and device\n",
    "__host__ __device__ void execution_location(const char* location) {\n",
    "#ifdef __CUDA_ARCH__\n",
    "    printf(\"Currently executing on: GPU (%s)\\n\", location);\n",
    "#else\n",
    "    printf(\"Currently executing on: CPU (%s)\\n\", location);\n",
    "#endif\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    \n",
    "    execution_location(\"???\");\n",
    "\n",
    "    thrust::for_each_n(thrust::device,\n",
    "                       thrust::counting_iterator<int>(0), 1,\n",
    "                       [=] __host__ __device__ (int) {\n",
    "                           execution_location(\"???\");\n",
    "                       });\n",
    "\n",
    "    \n",
    "    thrust::for_each_n(thrust::host,\n",
    "                       thrust::counting_iterator<int>(0), 1,\n",
    "                       [=] __host__ __device__ (int) {\n",
    "                           execution_location(\"???\");\n",
    "                       });\n",
    "\n",
    "    // Runs on CPU\n",
    "    execution_location(\"???\");\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef1d98-7204-4202-a590-cce06ac7782a",
   "metadata": {},
   "source": [
    "After filling in your answers, compile and run the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e53680-f142-4d18-8d50-2190dad03f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently executing on: CPU (???)\n",
      "Currently executing on: GPU (???)\n",
      "Currently executing on: CPU (???)\n",
      "Currently executing on: CPU (???)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvcc -o codes/particle_locations --extended-lambda codes/particle_locations.cu\n",
    "./codes/particle_locations              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25354ecb-d2e9-495d-a894-1a06e815157b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
